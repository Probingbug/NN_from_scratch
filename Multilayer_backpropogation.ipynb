{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOP8a7G0ZLgLa67+JbotS8s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Probingbug/NN_from_scratch/blob/main/Multilayer_backpropogation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9QhvJCvQsh8",
        "outputId": "df415f71-d4ea-422d-a30e-b54b4103e8aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 1: loss=240.25\n",
            "iteration 11: loss=8.827021031924428\n",
            "iteration 21: loss=1.630381905604496\n",
            "iteration 31: loss=0.4532586640113987\n",
            "iteration 41: loss=0.12600938209334828\n",
            "iteration 51: loss=0.035031573881063344\n",
            "iteration 61: loss=0.009739045999569202\n",
            "iteration 71: loss=0.002707529421993686\n",
            "iteration 81: loss=0.0007527139281697362\n",
            "final loss: 0.0002378378898668657\n",
            "final weights: [[-0.00710446 -0.01420892 -0.02131338 -0.02841784]\n",
            " [ 0.66820858  0.33641715  0.00462573 -0.3271657 ]\n",
            " [ 0.33221274  0.36442549  0.09663823 -0.37114903]]\n",
            "final biases: [-0.00710446 -0.03179142  0.13221274]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "inputs = [1,2,3,4]\n",
        "biases = [0.1,0.2,0.3]\n",
        "weights=[[0.1,0.2,0.3,0.4],[0.9,0.8,0.7,0.6],[0.5,0.7,0.6,0.3]]\n",
        "\n",
        "learning_rate = 0.001\n",
        "\n",
        "#activation function\n",
        "\n",
        "def relu(x):\n",
        "  return np.maximum(0,x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "   return np.where(x>0,1,0)\n",
        "\n",
        "# training loop\n",
        "\n",
        "for iteration in range ( 90):\n",
        "\n",
        "\n",
        "  # forward pass\n",
        "\n",
        "  z= np.dot(weights,inputs)+biases\n",
        "  a = relu(z)\n",
        "  y=np.sum(a,axis=0)\n",
        "\n",
        "  #calculate loss\n",
        "  loss = y**2\n",
        "\n",
        "  #backward pass\n",
        "\n",
        "\n",
        "  # Scalar to vector gradients\n",
        "  dl_dy = 2 * y                            # scalar\n",
        "  dy_da = np.ones_like(a)                 # shape: (3,)\n",
        "  dl_da = dl_dy * dy_da                   # shape: (3,)\n",
        "  da_dz = relu_derivative(z)              # shape: (3,)\n",
        "  dl_dz = dl_da * da_dz                   # shape: (3,)\n",
        "\n",
        "  # Now, compute outer product to match shape of weights (3, 4)\n",
        "  dl_dw = np.outer(dl_dz, inputs)         # shape: (3, 4)\n",
        "  dl_db = dl_dz     # shape: (3,)\n",
        "\n",
        "  #updation\n",
        "  weights -= learning_rate * dl_dw\n",
        "  biases -= learning_rate * dl_db\n",
        "\n",
        "\n",
        "  #print the loss\n",
        "\n",
        "  if iteration % 10 == 0:\n",
        "    print(f\"iteration {iteration+1}: loss={loss}\")\n",
        "\n",
        "#final loss,biases and weights\n",
        "\n",
        "print(f\"final loss: {loss}\")\n",
        "print(f\"final weights: {weights}\")\n",
        "print(f\"final biases: {biases}\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**softmax and cross entropy loss functions**"
      ],
      "metadata": {
        "id": "1Y2XVPHLeJjF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def activation_softmax():\n",
        "  def forward(self,inputs):\n",
        "    exp_values = np.exp(inputs - np.max(inputs,axis=1,keepdims=True))\n",
        "    probabilities = exp_values / np.sum(exp_values,axis=1,keepdims=True)\n",
        "\n",
        "def loss_categoricalCrossentropy():\n",
        "  def calculate(self,output,y):\n",
        "    samples = len(output)\n",
        "    output_clipped = np.clip(output,1e-7,1-1e-7)"
      ],
      "metadata": {
        "id": "eEjSKYHdd21B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Backward pass through softmax function.**\n",
        "combined softmax activation and cross-entropy loss for faster backward step"
      ],
      "metadata": {
        "id": "0jNqMt0QYhnD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# softmax classifier - combined softmax activation\n",
        "# and cross- entropy loss for faster backward step\n",
        "\n",
        "class activation_softmax_loss_categoricalCrossentropy():\n",
        "  #Creater activation and loss function objects\n",
        "\n",
        "  def __init__(self):\n",
        "    self.activation = activation_softmax()\n",
        "    self.loss = loss_categoricalCrossentropy()\n",
        "\n",
        "  # forward pass\n",
        "\n",
        "  def forward(self, inputs , y_true ):\n",
        "    # output layer's activation function\n",
        "\n",
        "    self.activation.forward(inputs)\n",
        "    #set the inputs\n",
        "    self.output = self.activation.output\n",
        "    #calculate and return loss value\n",
        "    return self.loss.calculate(self.output,y_true)\n",
        "\n",
        "\n",
        "  #backward pass\n",
        "\n",
        "  def backward(self,dvalues,y_true):\n",
        "    #numbers of samples\n",
        "    samples = len(dvalues)\n",
        "    #if labels are one-hot encoded,\n",
        "    #turn them into discrete values\n",
        "    if len(y_true.shape) == 2:\n",
        "      y_true = np.argmax(y_true,axis = 1) # when class targets are one hot encoded\n",
        "    #copy so we can modify safely\n",
        "    self.dinputs = dvalues.copy()\n",
        "    #calculate gradients\n",
        "    self.dinputs[range(samples),y_true] -=1. #only those block will be modify those are with y_true values\n",
        "    #normalize gradient\n",
        "    self.dinputs = self.dinputs / samples"
      ],
      "metadata": {
        "id": "5g5pcl0LSENf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "softmax_outputs = np.array ([[0.7,0.1,0.2],\n",
        "                             [0.1,0.5,0.4],\n",
        "                             [0.02,0.9,0.08]])\n",
        "\n",
        "class_targets = np.array([0,1,1])\n",
        "\n",
        "softmax_loss = activation_softmax_loss_categoricalCrossentropy()\n",
        "softmax_loss.backward(softmax_outputs,class_targets)\n",
        "dvalues = softmax_loss.dinputs\n",
        "print(\"gradients : combined loss and activation :\")\n",
        "print(dvalues)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "riRG0px3dTy7",
        "outputId": "ec24bc16-a13d-4f8c-bae0-917e664e091d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gradients : combined loss and activation :\n",
            "[[-0.1         0.03333333  0.06666667]\n",
            " [ 0.03333333 -0.16666667  0.13333333]\n",
            " [ 0.00666667 -0.03333333  0.02666667]]\n"
          ]
        }
      ]
    }
  ]
}