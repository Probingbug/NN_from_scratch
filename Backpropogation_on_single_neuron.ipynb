{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNmzC3Tj4/8LHwLGTtYhDwx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Probingbug/NN_from_scratch/blob/main/Backpropogation_on_single_neuron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQJdsneDk2e8",
        "outputId": "fa6d4d03-04fa-4793-dc31-35ada8dca9b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1 : loss : 36.0\n",
            "Iteration 2 : loss : 35.820225\n",
            "Iteration 3 : loss : 35.640900000000016\n",
            "Iteration 4 : loss : 35.46202500000001\n",
            "Iteration 5 : loss : 35.283600000000014\n",
            "Iteration 6 : loss : 35.10562500000003\n",
            "Iteration 7 : loss : 34.92810000000002\n",
            "Iteration 8 : loss : 34.751025000000034\n",
            "Iteration 9 : loss : 34.57440000000004\n",
            "Iteration 10 : loss : 34.398225000000046\n",
            "Iteration 11 : loss : 34.22250000000006\n",
            "Iteration 12 : loss : 34.047225000000054\n",
            "Iteration 13 : loss : 33.87240000000006\n",
            "Iteration 14 : loss : 33.69802500000007\n",
            "Iteration 15 : loss : 33.52410000000006\n",
            "Iteration 16 : loss : 33.35062500000008\n",
            "Iteration 17 : loss : 33.177600000000076\n",
            "Iteration 18 : loss : 33.00502500000008\n",
            "Iteration 19 : loss : 32.832900000000095\n",
            "Iteration 20 : loss : 32.66122500000009\n",
            "Iteration 21 : loss : 32.490000000000094\n",
            "Iteration 22 : loss : 32.31922500000011\n",
            "Iteration 23 : loss : 32.1489000000001\n",
            "Iteration 24 : loss : 31.979025000000114\n",
            "Iteration 25 : loss : 31.809600000000117\n",
            "Iteration 26 : loss : 31.64062500000012\n",
            "Iteration 27 : loss : 31.472100000000133\n",
            "Iteration 28 : loss : 31.304025000000127\n",
            "Iteration 29 : loss : 31.13640000000014\n",
            "Iteration 30 : loss : 30.969225000000144\n",
            "Iteration 31 : loss : 30.802500000000137\n",
            "Iteration 32 : loss : 30.63622500000015\n",
            "Iteration 33 : loss : 30.47040000000015\n",
            "Iteration 34 : loss : 30.305025000000157\n",
            "Iteration 35 : loss : 30.140100000000167\n",
            "Iteration 36 : loss : 29.97562500000016\n",
            "Iteration 37 : loss : 29.811600000000166\n",
            "Iteration 38 : loss : 29.648025000000178\n",
            "Iteration 39 : loss : 29.48490000000017\n",
            "Iteration 40 : loss : 29.322225000000184\n",
            "Iteration 41 : loss : 29.160000000000185\n",
            "Iteration 42 : loss : 28.99822500000019\n",
            "Iteration 43 : loss : 28.836900000000202\n",
            "Iteration 44 : loss : 28.676025000000195\n",
            "Iteration 45 : loss : 28.51560000000021\n",
            "Iteration 46 : loss : 28.35562500000021\n",
            "Iteration 47 : loss : 28.196100000000204\n",
            "Iteration 48 : loss : 28.037025000000217\n",
            "Iteration 49 : loss : 27.87840000000022\n",
            "Iteration 50 : loss : 27.72022500000022\n",
            "Iteration 51 : loss : 27.562500000000234\n",
            "Iteration 52 : loss : 27.405225000000225\n",
            "Iteration 53 : loss : 27.248400000000228\n",
            "Iteration 54 : loss : 27.09202500000024\n",
            "Iteration 55 : loss : 26.936100000000234\n",
            "Iteration 56 : loss : 26.780625000000246\n",
            "Iteration 57 : loss : 26.625600000000247\n",
            "Iteration 58 : loss : 26.471025000000253\n",
            "Iteration 59 : loss : 26.316900000000263\n",
            "Iteration 60 : loss : 26.163225000000256\n",
            "Iteration 61 : loss : 26.010000000000268\n",
            "Iteration 62 : loss : 25.85722500000027\n",
            "Iteration 63 : loss : 25.704900000000265\n",
            "Iteration 64 : loss : 25.553025000000275\n",
            "Iteration 65 : loss : 25.40160000000028\n",
            "Iteration 66 : loss : 25.25062500000028\n",
            "Iteration 67 : loss : 25.100100000000293\n",
            "Iteration 68 : loss : 24.950025000000284\n",
            "Iteration 69 : loss : 24.800400000000288\n",
            "Iteration 70 : loss : 24.6512250000003\n",
            "Iteration 71 : loss : 24.502500000000293\n",
            "Iteration 72 : loss : 24.3542250000003\n",
            "Iteration 73 : loss : 24.206400000000304\n",
            "Iteration 74 : loss : 24.059025000000307\n",
            "Iteration 75 : loss : 23.91210000000032\n",
            "Iteration 76 : loss : 23.765625000000313\n",
            "Iteration 77 : loss : 23.61960000000032\n",
            "Iteration 78 : loss : 23.474025000000324\n",
            "Iteration 79 : loss : 23.328900000000317\n",
            "Iteration 80 : loss : 23.18422500000033\n",
            "Iteration 81 : loss : 23.04000000000033\n",
            "Iteration 82 : loss : 22.89622500000033\n",
            "Iteration 83 : loss : 22.752900000000345\n",
            "Iteration 84 : loss : 22.610025000000338\n",
            "Iteration 85 : loss : 22.46760000000034\n",
            "Iteration 86 : loss : 22.32562500000035\n",
            "Iteration 87 : loss : 22.184100000000342\n",
            "Iteration 88 : loss : 22.043025000000352\n",
            "Iteration 89 : loss : 21.902400000000355\n",
            "Iteration 90 : loss : 21.762225000000356\n",
            "Iteration 91 : loss : 21.622500000000368\n",
            "Iteration 92 : loss : 21.48322500000036\n",
            "Iteration 93 : loss : 21.34440000000037\n",
            "Iteration 94 : loss : 21.206025000000373\n",
            "Iteration 95 : loss : 21.068100000000367\n",
            "Iteration 96 : loss : 20.930625000000376\n",
            "Iteration 97 : loss : 20.793600000000378\n",
            "Iteration 98 : loss : 20.657025000000377\n",
            "Iteration 99 : loss : 20.52090000000039\n",
            "Iteration 100 : loss : 20.385225000000382\n",
            "Iteration 101 : loss : 20.250000000000384\n",
            "Iteration 102 : loss : 20.115225000000393\n",
            "Iteration 103 : loss : 19.980900000000386\n",
            "Iteration 104 : loss : 19.847025000000396\n",
            "Iteration 105 : loss : 19.713600000000397\n",
            "Iteration 106 : loss : 19.5806250000004\n",
            "Iteration 107 : loss : 19.44810000000041\n",
            "Iteration 108 : loss : 19.3160250000004\n",
            "Iteration 109 : loss : 19.184400000000412\n",
            "Iteration 110 : loss : 19.053225000000413\n",
            "Iteration 111 : loss : 18.922500000000408\n",
            "Iteration 112 : loss : 18.792225000000414\n",
            "Iteration 113 : loss : 18.662400000000417\n",
            "Iteration 114 : loss : 18.533025000000418\n",
            "Iteration 115 : loss : 18.404100000000426\n",
            "Iteration 116 : loss : 18.27562500000042\n",
            "Iteration 117 : loss : 18.147600000000423\n",
            "Iteration 118 : loss : 18.02002500000043\n",
            "Iteration 119 : loss : 17.892900000000424\n",
            "Iteration 120 : loss : 17.766225000000432\n",
            "Iteration 121 : loss : 17.640000000000434\n",
            "Iteration 122 : loss : 17.514225000000437\n",
            "Iteration 123 : loss : 17.388900000000444\n",
            "Iteration 124 : loss : 17.264025000000437\n",
            "Iteration 125 : loss : 17.139600000000446\n",
            "Iteration 126 : loss : 17.015625000000448\n",
            "Iteration 127 : loss : 16.89210000000044\n",
            "Iteration 128 : loss : 16.76902500000045\n",
            "Iteration 129 : loss : 16.64640000000045\n",
            "Iteration 130 : loss : 16.524225000000452\n",
            "Iteration 131 : loss : 16.402500000000458\n",
            "Iteration 132 : loss : 16.281225000000454\n",
            "Iteration 133 : loss : 16.160400000000454\n",
            "Iteration 134 : loss : 16.040025000000462\n",
            "Iteration 135 : loss : 15.920100000000454\n",
            "Iteration 136 : loss : 15.80062500000046\n",
            "Iteration 137 : loss : 15.681600000000463\n",
            "Iteration 138 : loss : 15.563025000000469\n",
            "Iteration 139 : loss : 15.444900000000473\n",
            "Iteration 140 : loss : 15.327225000000466\n",
            "Iteration 141 : loss : 15.21000000000047\n",
            "Iteration 142 : loss : 15.093225000000475\n",
            "Iteration 143 : loss : 14.976900000000468\n",
            "Iteration 144 : loss : 14.861025000000472\n",
            "Iteration 145 : loss : 14.745600000000476\n",
            "Iteration 146 : loss : 14.63062500000048\n",
            "Iteration 147 : loss : 14.516100000000485\n",
            "Iteration 148 : loss : 14.402025000000478\n",
            "Iteration 149 : loss : 14.288400000000482\n",
            "Iteration 150 : loss : 14.175225000000486\n",
            "Iteration 151 : loss : 14.06250000000048\n",
            "Iteration 152 : loss : 13.950225000000483\n",
            "Iteration 153 : loss : 13.838400000000487\n",
            "Iteration 154 : loss : 13.727025000000491\n",
            "Iteration 155 : loss : 13.616100000000495\n",
            "Iteration 156 : loss : 13.505625000000489\n",
            "Iteration 157 : loss : 13.395600000000492\n",
            "Iteration 158 : loss : 13.286025000000496\n",
            "Iteration 159 : loss : 13.17690000000049\n",
            "Iteration 160 : loss : 13.068225000000492\n",
            "Iteration 161 : loss : 12.960000000000496\n",
            "Iteration 162 : loss : 12.8522250000005\n",
            "Iteration 163 : loss : 12.744900000000502\n",
            "Iteration 164 : loss : 12.638025000000496\n",
            "Iteration 165 : loss : 12.5316000000005\n",
            "Iteration 166 : loss : 12.425625000000503\n",
            "Iteration 167 : loss : 12.320100000000497\n",
            "Iteration 168 : loss : 12.2150250000005\n",
            "Iteration 169 : loss : 12.110400000000503\n",
            "Iteration 170 : loss : 12.006225000000507\n",
            "Iteration 171 : loss : 11.90250000000051\n",
            "Iteration 172 : loss : 11.799225000000504\n",
            "Iteration 173 : loss : 11.696400000000507\n",
            "Iteration 174 : loss : 11.59402500000051\n",
            "Iteration 175 : loss : 11.492100000000503\n",
            "Iteration 176 : loss : 11.390625000000506\n",
            "Iteration 177 : loss : 11.28960000000051\n",
            "Iteration 178 : loss : 11.189025000000512\n",
            "Iteration 179 : loss : 11.088900000000516\n",
            "Iteration 180 : loss : 10.98922500000051\n",
            "Iteration 181 : loss : 10.890000000000512\n",
            "Iteration 182 : loss : 10.791225000000514\n",
            "Iteration 183 : loss : 10.692900000000508\n",
            "Iteration 184 : loss : 10.595025000000511\n",
            "Iteration 185 : loss : 10.497600000000514\n",
            "Iteration 186 : loss : 10.400625000000517\n",
            "Iteration 187 : loss : 10.304100000000519\n",
            "Iteration 188 : loss : 10.208025000000513\n",
            "Iteration 189 : loss : 10.112400000000514\n",
            "Iteration 190 : loss : 10.017225000000517\n",
            "Iteration 191 : loss : 9.922500000000511\n",
            "Iteration 192 : loss : 9.828225000000513\n",
            "Iteration 193 : loss : 9.734400000000516\n",
            "Iteration 194 : loss : 9.641025000000518\n",
            "Iteration 195 : loss : 9.54810000000052\n",
            "Iteration 196 : loss : 9.455625000000515\n",
            "Iteration 197 : loss : 9.363600000000517\n",
            "Iteration 198 : loss : 9.272025000000518\n",
            "Iteration 199 : loss : 9.180900000000513\n",
            "Iteration 200 : loss : 9.090225000000515\n",
            "Iteration 201 : loss : 9.000000000000517\n",
            "Iteration 202 : loss : 8.91022500000052\n",
            "Iteration 203 : loss : 8.82090000000052\n",
            "Iteration 204 : loss : 8.732025000000515\n",
            "Iteration 205 : loss : 8.643600000000516\n",
            "Iteration 206 : loss : 8.555625000000518\n",
            "Iteration 207 : loss : 8.468100000000513\n",
            "Iteration 208 : loss : 8.381025000000514\n",
            "Iteration 209 : loss : 8.294400000000516\n",
            "Iteration 210 : loss : 8.208225000000517\n",
            "Iteration 211 : loss : 8.12250000000052\n",
            "Iteration 212 : loss : 8.037225000000513\n",
            "Iteration 213 : loss : 7.952400000000515\n",
            "Iteration 214 : loss : 7.868025000000516\n",
            "Iteration 215 : loss : 7.78410000000051\n",
            "Iteration 216 : loss : 7.700625000000512\n",
            "Iteration 217 : loss : 7.617600000000514\n",
            "Iteration 218 : loss : 7.535025000000515\n",
            "Iteration 219 : loss : 7.452900000000517\n",
            "Iteration 220 : loss : 7.371225000000511\n",
            "Iteration 221 : loss : 7.290000000000512\n",
            "Iteration 222 : loss : 7.209225000000513\n",
            "Iteration 223 : loss : 7.128900000000507\n",
            "Iteration 224 : loss : 7.049025000000508\n",
            "Iteration 225 : loss : 6.96960000000051\n",
            "Iteration 226 : loss : 6.890625000000511\n",
            "Iteration 227 : loss : 6.812100000000507\n",
            "Iteration 228 : loss : 6.734025000000506\n",
            "Iteration 229 : loss : 6.656400000000507\n",
            "Iteration 230 : loss : 6.579225000000508\n",
            "Iteration 231 : loss : 6.502500000000507\n",
            "Iteration 232 : loss : 6.426225000000503\n",
            "Iteration 233 : loss : 6.350400000000504\n",
            "Iteration 234 : loss : 6.275025000000505\n",
            "Iteration 235 : loss : 6.200100000000501\n",
            "Iteration 236 : loss : 6.1256250000004995\n",
            "Iteration 237 : loss : 6.051600000000501\n",
            "Iteration 238 : loss : 5.978025000000501\n",
            "Iteration 239 : loss : 5.9049000000005\n",
            "Iteration 240 : loss : 5.832225000000496\n",
            "Iteration 241 : loss : 5.760000000000496\n",
            "Iteration 242 : loss : 5.688225000000497\n",
            "Iteration 243 : loss : 5.616900000000493\n",
            "Iteration 244 : loss : 5.546025000000491\n",
            "Iteration 245 : loss : 5.475600000000492\n",
            "Iteration 246 : loss : 5.405625000000493\n",
            "Iteration 247 : loss : 5.33610000000049\n",
            "Iteration 248 : loss : 5.267025000000487\n",
            "Iteration 249 : loss : 5.198400000000487\n",
            "Iteration 250 : loss : 5.130225000000487\n",
            "Iteration 251 : loss : 5.062500000000483\n",
            "Iteration 252 : loss : 4.995225000000482\n",
            "Iteration 253 : loss : 4.928400000000482\n",
            "Iteration 254 : loss : 4.862025000000482\n",
            "Iteration 255 : loss : 4.7961000000004805\n",
            "Iteration 256 : loss : 4.730625000000477\n",
            "Iteration 257 : loss : 4.6656000000004765\n",
            "Iteration 258 : loss : 4.601025000000476\n",
            "Iteration 259 : loss : 4.536900000000473\n",
            "Iteration 260 : loss : 4.473225000000471\n",
            "Iteration 261 : loss : 4.41000000000047\n",
            "Iteration 262 : loss : 4.347225000000471\n",
            "Iteration 263 : loss : 4.2849000000004684\n",
            "Iteration 264 : loss : 4.223025000000464\n",
            "Iteration 265 : loss : 4.161600000000464\n",
            "Iteration 266 : loss : 4.100625000000464\n",
            "Iteration 267 : loss : 4.04010000000046\n",
            "Iteration 268 : loss : 3.980025000000458\n",
            "Iteration 269 : loss : 3.9204000000004573\n",
            "Iteration 270 : loss : 3.8612250000004567\n",
            "Iteration 271 : loss : 3.8025000000004536\n",
            "Iteration 272 : loss : 3.7442250000004504\n",
            "Iteration 273 : loss : 3.6864000000004498\n",
            "Iteration 274 : loss : 3.6290250000004494\n",
            "Iteration 275 : loss : 3.572100000000446\n",
            "Iteration 276 : loss : 3.5156250000004428\n",
            "Iteration 277 : loss : 3.4596000000004423\n",
            "Iteration 278 : loss : 3.4040250000004417\n",
            "Iteration 279 : loss : 3.3489000000004383\n",
            "Iteration 280 : loss : 3.294225000000435\n",
            "Iteration 281 : loss : 3.240000000000434\n",
            "Iteration 282 : loss : 3.1862250000004333\n",
            "Iteration 283 : loss : 3.13290000000043\n",
            "Iteration 284 : loss : 3.080025000000427\n",
            "Iteration 285 : loss : 3.027600000000426\n",
            "Iteration 286 : loss : 2.9756250000004245\n",
            "Iteration 287 : loss : 2.924100000000421\n",
            "Iteration 288 : loss : 2.873025000000418\n",
            "Iteration 289 : loss : 2.822400000000417\n",
            "Iteration 290 : loss : 2.772225000000416\n",
            "Iteration 291 : loss : 2.7225000000004123\n",
            "Iteration 292 : loss : 2.673225000000409\n",
            "Iteration 293 : loss : 2.6244000000004077\n",
            "Iteration 294 : loss : 2.5760250000004064\n",
            "Iteration 295 : loss : 2.5281000000004026\n",
            "Iteration 296 : loss : 2.480625000000399\n",
            "Iteration 297 : loss : 2.4336000000003977\n",
            "Iteration 298 : loss : 2.3870250000003965\n",
            "Iteration 299 : loss : 2.340900000000393\n",
            "Iteration 300 : loss : 2.2952250000003893\n",
            "Iteration 301 : loss : 2.2500000000003877\n",
            "Iteration 302 : loss : 2.205225000000386\n",
            "Iteration 303 : loss : 2.1609000000003826\n",
            "Iteration 304 : loss : 2.1170250000003787\n",
            "Iteration 305 : loss : 2.0736000000003774\n",
            "Iteration 306 : loss : 2.0306250000003754\n",
            "Iteration 307 : loss : 1.9881000000003717\n",
            "Iteration 308 : loss : 1.946025000000368\n",
            "Iteration 309 : loss : 1.9044000000003662\n",
            "Iteration 310 : loss : 1.8632250000003643\n",
            "Iteration 311 : loss : 1.8225000000003606\n",
            "Iteration 312 : loss : 1.7822250000003568\n",
            "Iteration 313 : loss : 1.7424000000003548\n",
            "Iteration 314 : loss : 1.7030250000003528\n",
            "Iteration 315 : loss : 1.664100000000349\n",
            "Iteration 316 : loss : 1.6256250000003452\n",
            "Iteration 317 : loss : 1.587600000000343\n",
            "Iteration 318 : loss : 1.5500250000003408\n",
            "Iteration 319 : loss : 1.512900000000337\n",
            "Iteration 320 : loss : 1.4762250000003332\n",
            "Iteration 321 : loss : 1.4400000000003308\n",
            "Iteration 322 : loss : 1.4042250000003285\n",
            "Iteration 323 : loss : 1.3689000000003246\n",
            "Iteration 324 : loss : 1.3340250000003206\n",
            "Iteration 325 : loss : 1.2996000000003183\n",
            "Iteration 326 : loss : 1.2656250000003157\n",
            "Iteration 327 : loss : 1.2321000000003117\n",
            "Iteration 328 : loss : 1.1990250000003078\n",
            "Iteration 329 : loss : 1.1664000000003052\n",
            "Iteration 330 : loss : 1.1342250000003027\n",
            "Iteration 331 : loss : 1.1025000000002985\n",
            "Iteration 332 : loss : 1.0712250000002945\n",
            "Iteration 333 : loss : 1.0404000000002918\n",
            "Iteration 334 : loss : 1.010025000000289\n",
            "Iteration 335 : loss : 0.980100000000284\n",
            "Iteration 336 : loss : 0.95062500000028\n",
            "Iteration 337 : loss : 0.9216000000002762\n",
            "Iteration 338 : loss : 0.8930250000002714\n",
            "Iteration 339 : loss : 0.8649000000002676\n",
            "Iteration 340 : loss : 0.8372250000002636\n",
            "Iteration 341 : loss : 0.8100000000002596\n",
            "Iteration 342 : loss : 0.7832250000002556\n",
            "Iteration 343 : loss : 0.7569000000002509\n",
            "Iteration 344 : loss : 0.731025000000247\n",
            "Iteration 345 : loss : 0.705600000000243\n",
            "Iteration 346 : loss : 0.6806250000002383\n",
            "Iteration 347 : loss : 0.6561000000002343\n",
            "Iteration 348 : loss : 0.6320250000002302\n",
            "Iteration 349 : loss : 0.6084000000002262\n",
            "Iteration 350 : loss : 0.5852250000002222\n",
            "Iteration 351 : loss : 0.5625000000002175\n",
            "Iteration 352 : loss : 0.5402250000002135\n",
            "Iteration 353 : loss : 0.5184000000002094\n",
            "Iteration 354 : loss : 0.4970250000002047\n",
            "Iteration 355 : loss : 0.47610000000020064\n",
            "Iteration 356 : loss : 0.45562500000019657\n",
            "Iteration 357 : loss : 0.43560000000019244\n",
            "Iteration 358 : loss : 0.4160250000001884\n",
            "Iteration 359 : loss : 0.39690000000018366\n",
            "Iteration 360 : loss : 0.37822500000017956\n",
            "Iteration 361 : loss : 0.36000000000017546\n",
            "Iteration 362 : loss : 0.34222500000017075\n",
            "Iteration 363 : loss : 0.3249000000001666\n",
            "Iteration 364 : loss : 0.3080250000001625\n",
            "Iteration 365 : loss : 0.2916000000001583\n",
            "Iteration 366 : loss : 0.2756250000001541\n",
            "Iteration 367 : loss : 0.2601000000001495\n",
            "Iteration 368 : loss : 0.2450250000001453\n",
            "Iteration 369 : loss : 0.2304000000001411\n",
            "Iteration 370 : loss : 0.21622500000013647\n",
            "Iteration 371 : loss : 0.20250000000013224\n",
            "Iteration 372 : loss : 0.18922500000012804\n",
            "Iteration 373 : loss : 0.1764000000001238\n",
            "Iteration 374 : loss : 0.16402500000011955\n",
            "Iteration 375 : loss : 0.15210000000011492\n",
            "Iteration 376 : loss : 0.14062500000011066\n",
            "Iteration 377 : loss : 0.12960000000010638\n",
            "Iteration 378 : loss : 0.11902500000010179\n",
            "Iteration 379 : loss : 0.1089000000000975\n",
            "Iteration 380 : loss : 0.0992250000000932\n",
            "Iteration 381 : loss : 0.09000000000008888\n",
            "Iteration 382 : loss : 0.08122500000008456\n",
            "Iteration 383 : loss : 0.07290000000007998\n",
            "Iteration 384 : loss : 0.06502500000007565\n",
            "Iteration 385 : loss : 0.057600000000071296\n",
            "Iteration 386 : loss : 0.050625000000066735\n",
            "Iteration 387 : loss : 0.044100000000062374\n",
            "Iteration 388 : loss : 0.038025000000058\n",
            "Iteration 389 : loss : 0.032400000000053615\n",
            "Iteration 390 : loss : 0.027225000000049217\n",
            "Iteration 391 : loss : 0.022500000000044672\n",
            "Iteration 392 : loss : 0.01822500000004026\n",
            "Iteration 393 : loss : 0.014400000000035837\n",
            "Iteration 394 : loss : 0.011025000000031308\n",
            "Iteration 395 : loss : 0.008100000000026872\n",
            "Iteration 396 : loss : 0.005625000000022425\n",
            "Iteration 397 : loss : 0.0036000000000179656\n",
            "Iteration 398 : loss : 0.002025000000013493\n",
            "Iteration 399 : loss : 0.0009000000000089811\n",
            "Iteration 400 : loss : 0.0002250000000044968\n",
            "Final weights : [-3.4 -0.2  0.8]\n",
            "Final bias : 0.5999999999999996\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "#initial parameters\n",
        "\n",
        "weights = np.array([-3.0,-1.0,2.0])\n",
        "bias =1.0\n",
        "inputs = np.array([1.0,-2.0,3.0])\n",
        "target_output = 0.0\n",
        "target_rate = 0.001\n",
        "\n",
        "def relu(x):\n",
        "  return np.maximum(0,x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "  if x>1:\n",
        "    return 1\n",
        "  else : return 0\n",
        "\n",
        "for iteration in range (400):\n",
        "  #forward pass\n",
        "  linear_output = np.dot(weights,inputs) + bias\n",
        "  output = relu(linear_output)\n",
        "  loss = (output - target_output)**2\n",
        "\n",
        "  #backward pass\n",
        "  dloss_dout = 2*(output - target_output)\n",
        "  dout_dlinear_output = relu_derivative(linear_output)\n",
        "  dlinear_output_dweights = inputs\n",
        "  dlinear_output_dbias = 1.0\n",
        "\n",
        "  #updation of weights and the biases\n",
        "\n",
        "  weights -= target_rate * dlinear_output_dweights\n",
        "  bias -= target_rate *  dlinear_output_dbias\n",
        "\n",
        "  print(f\"Iteration {iteration +1 } : loss : {loss }\")\n",
        "\n",
        "\n",
        "print(f\"Final weights : {weights}\")\n",
        "print(f\"Final bias : {bias}\")\n"
      ]
    }
  ]
}